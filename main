from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, current_date, lit, to_timestamp, regexp_replace, date_add
from pyspark.sql.types import IntegerType, StringType, TimestampType
from airflow.hooks.postgres_hook import PostgresHook

default_args = {
    "owner": "Mixanik",
    "depends_on_past": False,
    "retries": 1,
    'start_date':datetime(2024, 10, 1),#тестовые данные для проверки работоспособности, можно оставить только старт дейт + поставить catchup=False
    'end_date':datetime(2025, 2, 1),
    "retry_delay": timedelta(minutes=2)
}

def spark_session():
    spark = SparkSession.builder \
        .appName("SparkHiveExample") \
        .config("spark.jars", "/path/to/postgresql-42.7.2.jar")\
        .enableHiveSupport() \
        .getOrCreate()
    return spark


def data_to_hive(ti, **context):
    data_interval_start = context['data_interval_start'].strftime('%Y-%m-%d')
    print(data_interval_start)
    data_interval_end = context['data_interval_end'].strftime('%Y-%m-%d')
    print(data_interval_end)
    spark = spark_session()
    path_to_file='/user/spark/1'

    nashville_df = (spark.read.csv(path_to_file, header=True, inferSchema=True))
    nashville_df = nashville_df.withColumn("Date and Time",regexp_replace(col("Date and Time"), ",\s*", ""))
    nashville_df = nashville_df.withColumn("Date and Time", to_timestamp(col("Date and Time"), "M/d/yyyy h:mm:ss a"))
    nashville_df = nashville_df.filter(col("Date and Time").between(lit(data_interval_start), date_add(lit(data_interval_end), -1)))
    rename_map = {
        "Accident Number": "AccidentNumber",
        "Street Address": "StreetAddress",
        "x": "Xcoordinate",
        "y": "Ycoordinate",
        "Zip code": "Zip_code",
        "Weather Description": "WeatherDescription",
        "Illumination Description": "IlluminationDescription",
        "Collision Type": "CollisionType",
        "Reporting Officer": "ReportingOfficer",
        "Collision Type Description": "CollisionTypeDescription",
        "Hit and Run": "Hit_and_Run",
        "Property Damage": "PropertyDamage",
        "Date and Time": "Date_and_Time",
        "Number of Motor Vehicles": "Motors",
        "Number of Injuries": "Injuries",
        "Number of Fatalities": "Fatalities"
    }
    for old_col, new_col in rename_map.items():
        nashville_df = nashville_df.withColumnRenamed(old_col, new_col)


    nashville_df = nashville_df.withColumn("AccidentNumber", col("AccidentNumber").cast(IntegerType()))


    accident_info_columns = ["AccidentNumber", "Date_and_Time", "Hit_and_Run", "Motors",
        "Injuries", "Fatalities", "PropertyDamage",
        "CollisionTypeDescription", "HarmfulDescriptions", "CollisionType", "ReportingOfficer", "RPA"]


    location_columns =["AccidentNumber", "StreetAddress", "City", "State",
                       "Precinct", "Lat", "Long", "Xcoordinate", "Ycoordinate",
                       "Zip_code", "ObjectId"]

    factors_columns = ["AccidentNumber", "WeatherDescription", "IlluminationDescription",
                    "Weather", "IlluACCIDEmination"]


    nashville_df.select(*accident_info_columns).write.format("hive").mode('append').saveAsTable("nashville.accident_info")
    nashville_df.select(*location_columns).write.format("hive").mode('append').saveAsTable("nashville.location")
    nashville_df.select(*factors_columns).write.format("hive").mode('append').saveAsTable("nashville.factores")

    result = spark.sql("SELECT COUNT(*) FROM nashville.accident_info")
    result.show()

    ti.xcom_push(key="data_to_hive", value=f"hive insert {nashville_df.count()} successfully")

def use_connection(ti, **context):
    conn = PostgresHook.get_connection("hm3_db")
    print(
        f"Host: {conn.host}, Login: {conn.login}, Password: {conn.password}, Port: {conn.port}"
    )
    ti.xcom_push(key="connection_status", value="Success")
def from_hive_to_postgre(ti,**context):
    data_interval_start = context['data_interval_start'].strftime('%Y-%m-%d')
    print(data_interval_start)
    data_interval_end = context['data_interval_end'].strftime('%Y-%m-%d')
    print(data_interval_end)
    spark = spark_session()

    accident_info_df = spark.sql("SELECT * FROM nashville.accident_info ").filter(col("Date_and_Time").between(lit(data_interval_start), date_add(lit(data_interval_end), -1)))
    location_df = spark.sql("SELECT * FROM nashville.location").filter(col("Date_and_Time").between(lit(data_interval_start), date_add(lit(data_interval_end), -1)))
    factors_df = spark.sql("SELECT * FROM nashville.factors").filter(col("Date_and_Time").between(lit(data_interval_start), date_add(lit(data_interval_end), -1)))

    accident_info_df.write.jdbc(url="jdbc:postgresql://158.160.173.243:5432/hm3", table="accident_info", mode="overwrite",
                       properties={"user": "admin", "password": "admin", "driver":"org.postgresql.Driver"})
    location_df.write.jdbc(url="jdbc:postgresql://158.160.173.243:5432/hm3", table="location", mode="overwrite",
                                properties={"user": "admin", "password": "admin", "driver":"org.postgresql.Driver"})
    factors_df.write.jdbc(url="jdbc:postgresql://158.160.173.243:5432/hm3", table="factors", mode="overwrite",
                                properties={"user": "admin", "password": "admin", "driver":"org.postgresql.Driver"})

    ti.xcom_push(key="load_status", value="Success")
with DAG(
    "data_from_csv_to_psql",
    default_args=default_args,
    description="DAG from csv to PostgreSQL",
    schedule_interval='@monthly',
    catchup=True,
) as dag:

    load_data_to_hive = PythonOperator(
        task_id="data_to_hive",
        python_callable=data_to_hive,
        provide_context=True
    )

    conn_to_postgre=PythonOperator(
        task_id="postgre_connection",
        python_callable=use_connection,
        provide_context=True
    )

    from_hive_to_postgre = PythonOperator(
        task_id="from_hive_to_postgre",
        python_callable=from_hive_to_postgre,
        provide_context=True
    )



    load_data_to_hive >> conn_to_postgre >> from_hive_to_postgre
